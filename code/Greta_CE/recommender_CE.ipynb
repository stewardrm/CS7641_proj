{"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive/', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzBUE2u83gWq","executionInfo":{"status":"ok","timestamp":1681345201955,"user_tz":420,"elapsed":2501,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}},"outputId":"343d3db7-6de2-4d5c-9c5a-ede8c60c2ad8"},"id":"vzBUE2u83gWq","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/Anime_Recommender/Greta_CE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-r6PB0Q3l3Y","executionInfo":{"status":"ok","timestamp":1681345201956,"user_tz":420,"elapsed":10,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}},"outputId":"11c98b54-037b-47c6-a4a4-97c9f2baad72"},"id":"Q-r6PB0Q3l3Y","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Anime_Recommender/Greta_CE\n"]}]},{"cell_type":"code","source":["!pip install sentence_transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tunBDAV13sS9","executionInfo":{"status":"ok","timestamp":1681345207764,"user_tz":420,"elapsed":5813,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}},"outputId":"4925a507-3dab-4a00-978f-08e4edd4f12f"},"id":"tunBDAV13sS9","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.13.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.27.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.1.98)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3240,"status":"ok","timestamp":1681345211001,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"fb4f1fab-21fe-4afd-912d-1356992c8d7a"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer, util\n","from transformers import AutoTokenizer, AutoModel\n","from sentence_transformers import CrossEncoder\n","from pathlib import Path\n","import csv\n","import pickle\n","import time\n","import sys\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"fb4f1fab-21fe-4afd-912d-1356992c8d7a"},{"cell_type":"code","execution_count":5,"metadata":{"id":"0515db63-0c13-4f5a-a33f-d100322d3397","executionInfo":{"status":"ok","timestamp":1681345211001,"user_tz":420,"elapsed":7,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}}},"outputs":[],"source":["# Dataset we are to use\n","dataset_path = \"./data/animes.csv\"\n"],"id":"0515db63-0c13-4f5a-a33f-d100322d3397"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1681345211001,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"d7da9362-118c-408f-a70c-a56bf8c3f246","outputId":"bde9624e-ef79-4090-d0c3-78b8de6a923b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'anime-embeddings.pkl'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["# File for caching  embeddings\n","embedding_cache_path = 'anime-embeddings.pkl'\n","df_cache_path = 'df-anime.pkl'\n","embedding_cache_path"],"id":"d7da9362-118c-408f-a70c-a56bf8c3f246"},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":496,"status":"ok","timestamp":1681345211492,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"bfb125b2-53fb-40ff-9d28-aeb13c109021"},"outputs":[],"source":["# BiEncoder (SentenceTransformer) that produces embeddings for input request and uses cosine similarity to filter top num_candidates similar synposes.\n","bi_encoder_model = SentenceTransformer('all-MiniLM-L6-v2')\n","num_candidates = 500\n","max_corpus_size = 20000"],"id":"bfb125b2-53fb-40ff-9d28-aeb13c109021"},{"cell_type":"code","source":["#Mean Pooling - Take attention mask into account for correct averaging - NOT in use\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"],"metadata":{"id":"ongs6n4i5ksm","executionInfo":{"status":"ok","timestamp":1681345211493,"user_tz":420,"elapsed":5,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}}},"id":"ongs6n4i5ksm","execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2197,"status":"ok","timestamp":1681345213686,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"7cc9fd5a-e295-451b-9f89-4ef10898f183","outputId":"abd3b976-2c33-4ff1-b49d-39630ada3bd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Load pre-computed embeddings from disc\n"]}],"source":["#Check if embedding cache path exists. If not, embed synopses using BiEncoder\n","cached_file = Path(embedding_cache_path)\n","\n","if cached_file.exists():\n","    df = pd.read_pickle(df_cache_path)  \n","    corpus_sentences = df['synopsis'].tolist()\n","    print(\"Load pre-computed embeddings from disc\")\n","    with open(embedding_cache_path, \"rb\") as fIn:\n","        cache_data = pickle.load(fIn)\n","    corpus_embeddings = cache_data['embeddings'][0:max_corpus_size]\n","\n","else:\n","    df = pd.read_csv(dataset_path)\n","    df.dropna(subset=['synopsis'], inplace=True)\n","    df.drop_duplicates(subset='synopsis', keep='first', inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","\n","    corpus_sentences = df['synopsis'].tolist()\n","\n","    \"\"\"\n","    # Load model from HuggingFace Hub\n","    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n","    model_test = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n","\n","    # Tokenize sentences\n","    encoded_input = tokenizer(corpus_sentences, padding=True, truncation=True, return_tensors='pt')\n","\n","    # Compute token embeddings\n","    with torch.no_grad():\n","        model_output = model_test(**encoded_input)\n","\n","    # Perform pooling. In this case, max pooling.\n","    corpus_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","    \"\"\"\n","    \n","    print(\"Encoding synopses\")\n","    corpus_embeddings = bi_encoder_model.encode(corpus_sentences, show_progress_bar=True, convert_to_tensor=True)\n","\n","    print(\"Store file on disc\")\n","    df.to_pickle(df_cache_path)  \n","    with open(embedding_cache_path, \"wb\") as fOut:\n","        pickle.dump({'sentences': corpus_sentences, 'embeddings': corpus_embeddings}, fOut) \n"],"id":"7cc9fd5a-e295-451b-9f89-4ef10898f183"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1681345213686,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"SYPwA9zXV4Q8","outputId":"1829f3fc-5bed-46aa-d299-29ca7e4fb057"},"outputs":[{"output_type":"stream","name":"stdout","text":["15191\n"]}],"source":["print(len(df))"],"id":"SYPwA9zXV4Q8"},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1681345213687,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"},"user_tz":420},"id":"7f517d1f-8577-4574-ad4c-fea32535d443","outputId":"a5ff7641-acb6-4225-c5f4-e3db04c9b0c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["15191 sentences / 15191 embeddings configured\n"]}],"source":["print(f\"{len(df)} sentences / {len(corpus_embeddings)} embeddings configured\")"],"id":"7f517d1f-8577-4574-ad4c-fea32535d443"},{"cell_type":"code","execution_count":12,"metadata":{"id":"c7818efa-3559-4aa7-8568-f0752a69f28d","executionInfo":{"status":"ok","timestamp":1681345215905,"user_tz":420,"elapsed":2222,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}}},"outputs":[],"source":["# Use the filtered num_candidates synopses as inout to CrossEncoder. A CrossEncoder gets both inputs (input request, synopsis) and outputs similarity score\n","CE_model = 'cross-encoder/stsb-roberta-base'\n","cross_encoder_model = CrossEncoder(CE_model)"],"id":"c7818efa-3559-4aa7-8568-f0752a69f28d"},{"cell_type":"code","execution_count":13,"metadata":{"id":"pzloe-4cHMsb","executionInfo":{"status":"ok","timestamp":1681345215906,"user_tz":420,"elapsed":14,"user":{"displayName":"Greta Sharoyan","userId":"04959894080622396726"}}},"outputs":[],"source":["def cos_sim(a, b):\n","    \"\"\"\n","    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n","    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n","    \"\"\"\n","    if not isinstance(a, torch.Tensor):\n","        a = torch.tensor(a)\n","\n","    if not isinstance(b, torch.Tensor):\n","        b = torch.tensor(b)\n","\n","    if len(a.shape) == 1:\n","        a = a.unsqueeze(0)\n","\n","    if len(b.shape) == 1:\n","        b = b.unsqueeze(0)\n","\n","    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n","    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n","    return torch.mm(a_norm, b_norm.transpose(0, 1).to(device))"],"id":"pzloe-4cHMsb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"da72ae06-6c95-4bf7-ab59-c5a7859de0df","outputId":"a96263b4-4701-440e-9373-af7ea3c5be95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Please enter a request: Historical fiction about vikings and revenge\n","Input request: Historical fiction about vikings and revenge\n","Cosine-Similarity search took 0.504 seconds\n","Top 5 hits with cosine-similarity:\n","\t0.633\tVinland Saga\n","\t0.459\tHimekishi Angelica\n","\t0.446\tSword Art Online: Alicization - War of Underworld 2nd Season\n","\t0.432\tShadow Skill: Kurudaryuu Kousatsuhou no Himitsu\n","\t0.429\tDoupo Cangqiong 2nd Season Specials\n","\n","Re-ranking with Cross-Encoder took 7.346 seconds\n","Top 5 hits with CrossEncoder:\n","\t0.565\tKami nomi zo Shiru Sekai\n","\t0.534\tTales of Phantasia The Animation\n","\t0.510\tChuan Shu Zijiu Zhinan\n","\t0.508\tFeng Yun Jue\n","\t0.490\tArslan Senki\n","\n","\n","========\n","\n"]}],"source":["while True:\n","    inp_request = input(\"Please enter a request: \")\n","    print(\"Input request:\", inp_request)\n","\n","    #First, filter num_candidates using bi-encoder\n","    start_time = time.time()\n","    question_embedding = bi_encoder_model.encode(inp_request, convert_to_tensor=True)\n","    \n","    #Get top 5 results from bi-encoder\n","    #semantic_search performs similarity followed by topk filtering\n","    encoder_res = util.semantic_search(question_embedding, corpus_embeddings, top_k=num_candidates)[0]\n","        #Check that corpus and queries are on the same device\n","    # if corpus_embeddings.device != question_embedding.device:\n","    #     query_embeddings = question_embedding.to(corpus_embeddings.device)\n","    # cos_scores = cos_sim(question_embedding, corpus_embeddings)[0]\n","\n","    # top_results = torch.topk(cos_scores, k=num_candidates)\n","\n","\n","\n","    print(\"Cosine-Similarity search took {:.3f} seconds\".format(time.time()-start_time))\n","    print(\"Top 5 hits with cosine-similarity:\")\n","    for res in encoder_res[0:5]:\n","        # print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n","        print(\"\\t{:.3f}\\t{}\".format(res['score'], df['title'].iloc[res['corpus_id']]))\n","        # print(\"\\t\",df['title'].iloc[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n","\n","    #Use filtered results form bi-encoder as input to cross-encoder\n","    start_time = time.time()\n","    sentence_pairs = [[inp_request, corpus_sentences[res['corpus_id']]] for res in encoder_res] #pair all summaries with input request\n","    ce_scores = cross_encoder_model.predict(sentence_pairs)\n","\n","\n","    for i in range(len(encoder_res)):\n","        encoder_res[i]['cross-encoder_score'] = ce_scores[i]\n","\n","    #Sort CrossEncoder output results by scores\n","    encoder_res = sorted(encoder_res, key=lambda x: x['cross-encoder_score'], reverse=True)\n","    print(\"\\nRe-ranking with Cross-Encoder took {:.3f} seconds\".format(time.time() - start_time))\n","    print(\"Top 5 hits with CrossEncoder:\")\n","    for res in encoder_res[0:5]:\n","        # print(\"\\t{:.3f}\\t{}\".format(hit['cross-encoder_score'], corpus_sentences[hit['corpus_id']]))\n","        print(\"\\t{:.3f}\\t{}\".format(res['cross-encoder_score'], df['title'].iloc[res['corpus_id']]))\n","    print(\"\\n\\n========\\n\")"],"id":"da72ae06-6c95-4bf7-ab59-c5a7859de0df"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}